{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import MNISTtools\n",
    "\n",
    "xtrain, ltrain = MNISTtools.load(dataset='training', path = '/datasets/MNIST')\n",
    "print(xtrain.shape)\n",
    "print(ltrain.shape)\n",
    "MNISTtools.show(xtrain[:,42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_MNIST_images(x):\n",
    "    x = x.astype(np.float64)\n",
    "    x = (x - 127.5)/127.5\n",
    "    return x\n",
    "\n",
    "xtrain = normalize_MNIST_images(xtrain)\n",
    "\n",
    "def label2onehot(lbl):\n",
    "    d = np.zeros((lbl.max() + 1, lbl.size))\n",
    "    d[lbl[np.arange(0, lbl.size)], np.arange(0, lbl.size)] = 1\n",
    "    return d\n",
    "\n",
    "dtrain = label2onehot(ltrain)\n",
    "\n",
    "def onehot2label(d):\n",
    "    lbl = d.argmax(axis=0)\n",
    "    return lbl\n",
    "\n",
    "def softmax(a):\n",
    "    M = a.max(axis = 0)\n",
    "    func = np.exp(a - M)/np.sum(np.exp(a - M), axis = 0)\n",
    "    return func\n",
    "\n",
    "def softmaxp(a, e):\n",
    "    s = softmax(a)\n",
    "    x = np.multiply(s,e)\n",
    "    y = np.multiply(np.sum(x, axis = 0),s)\n",
    "    delta = x - y\n",
    "    return delta\n",
    "\n",
    "def relu(a):\n",
    "    return np.where(a <= 0, 0, a)\n",
    "\n",
    "def relup(a, e):\n",
    "    r = relu(a)\n",
    "    X1 = np.where(r == 0, 0,1)\n",
    "    delta = np.multiply(X1,e)\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify the implementation of softmaxp and relup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps= 1e-6\n",
    "a= np.random.randn(10, 200)\n",
    "e= np.random.randn(10, 200)\n",
    "diff= softmaxp(a, e)\n",
    "diff_approx = (softmax(a + eps*e) - softmax(a))/eps\n",
    "rel_error= np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "print(rel_error, ' should be smaller than 1e-6 ' )\n",
    "\n",
    "\n",
    "eps= 1e-6\n",
    "a= np.random.randn(10, 200)\n",
    "e= np.random.randn(10, 200)\n",
    "diff= relup(a, e)\n",
    "diff_approx = (relu(a + eps*e) - relu(a))/eps\n",
    "rel_error= np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "print(rel_error, ' should be smaller than 1e-6 ' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for initialization, forward prop , cross entropy loss and performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_shallow(Ni, Nh, No):\n",
    "    b1 = np.random.randn(Nh, 1) / np.sqrt((Ni+1.)/2.)\n",
    "    W1 = np.random.randn(Nh, Ni) / np.sqrt((Ni+1.)/2.)\n",
    "    b2 = np.random.randn(No, 1) / np.sqrt((Nh+1.))\n",
    "    W2 = np.random.randn(No, Nh) / np.sqrt((Nh+1.))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "Ni = xtrain.shape[0]\n",
    "Nh = 64\n",
    "No = dtrain.shape[0]\n",
    "netinit = init_shallow(Ni, Nh, No)\n",
    "\n",
    "def forwardprop_shallow(x, net):\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    a1 = W1.dot(x) + b1\n",
    "    h1 = relu(a1)\n",
    "    a2 = W2.dot(h1) + b2\n",
    "    y = softmax(a2)\n",
    "    return y\n",
    "\n",
    "def eval_loss(y, d):\n",
    "    return -np.mean((d*np.log(y)))\n",
    "\n",
    "def eval_perfs(y, lbl):\n",
    "    d = np.zeros((1, lbl.size))\n",
    "    return 100*(np.sum(onehot2label(y[:,np.arange(0,lbl.size)]) != lbl[np.arange(0, lbl.size)]).astype(np.float64)/lbl.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yinit = forwardprop_shallow(xtrain, netinit)\n",
    "print(eval_loss(yinit, dtrain), ' should be around .26 ' )\n",
    "print(eval_perfs(yinit, ltrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### backprop implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_shallow(x, d, net, gamma=.05):\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    Ni = W1.shape[1]\n",
    "    Nh = W1.shape[0]\n",
    "    No = W2.shape[0]\n",
    "    gamma = gamma / x.shape[1] # normalized by the training dataset size\n",
    "        \n",
    "    a1 = W1.dot(x) + b1\n",
    "    z1 = relu(a1)\n",
    "    a2 = W2.dot(z1) + b2\n",
    "    y = softmax(a2)\n",
    "    \n",
    "    dw2 = softmaxp(a2,-1*(d/y))\n",
    "    dw1 = relup(a1,(np.dot(W2.T,dw2)))\n",
    "    \n",
    "    W2 -= np.dot(dw2,z1.T)*gamma\n",
    "    W1 -= np.dot(dw1, x.T)*gamma\n",
    "    b2 -= np.sum(dw2,axis=1, keepdims=True)*gamma\n",
    "    b1 -= np.sum(dw1,axis=1, keepdims=True)*gamma\n",
    "        \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def backprop_shallow(x, d, net, T, gamma=.05):\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(0, T):\n",
    "        net = update_shallow(x,d,net,gamma)\n",
    "        y = forwardprop_shallow(x, net)\n",
    "        print('Loss is', eval_loss(y, dtrain))\n",
    "        print('Miss-Clasifications', eval_perfs(y,lbl))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nettrain = backprop_shallow(xtrain, dtrain, netinit, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction on test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest, ltest = MNISTtools.load(dataset='testing', path = '/datasets/MNIST')\n",
    "print(xtest.shape)\n",
    "print(ltest.shape)\n",
    "\n",
    "xtest = normalize_MNIST_images(xtest)\n",
    "dtest = label2onehot(ltest)\n",
    "\n",
    "y_predict = forwardprop_shallow(xtest, nettrain)\n",
    "print('Test Loss is', eval_loss(y_predict, dtest))\n",
    "print('Test Miss-Clasifications is', eval_perfs(y_predict,ltest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minibatch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_minibatch_shallow(x, d, net, T, B=100, gamma=.05):\n",
    "    N = x.shape[1]\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(0, T):\n",
    "        for l in range(0, (N+B-1)/B):\n",
    "            idx = np.arange(B*l,min(B*(l+1), N))\n",
    "            net = update_shallow(x[:,idx] , d[:,idx], net , gamma)\n",
    "        y = forwardprop_shallow(x, net)\n",
    "        print('Loss is', eval_loss(y, dtrain))\n",
    "        print('Miss-Clasifications', eval_perfs(y,lbl))\n",
    "    return net\n",
    "\n",
    "Ni = xtrain.shape[0]\n",
    "Nh = 64\n",
    "No = dtrain.shape[0]\n",
    "netinit = init_shallow(Ni, Nh, No)\n",
    "\n",
    "netminibatch = backprop_minibatch_shallow(xtrain, dtrain, netinit, 5, B=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = forwardprop_shallow(xtest, netminibatch)\n",
    "print('Test Loss is', eval_loss(y_predict, dtest))\n",
    "print('Test Miss-Clasifications is', eval_perfs(y_predict,ltest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
