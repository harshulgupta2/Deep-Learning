{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries and Enable GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import MNISTtools\n",
    "import torch.utils.data as data_utils\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import features and labels to numpy arrays. \n",
    "#### Normalize images between [-1,1] \n",
    "#### Convert labels to one-hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "xtrain, ltrain = MNISTtools.load(dataset='training', path = '/datasets/MNIST')\n",
    "xtrain = np.transpose(xtrain)\n",
    "\n",
    "def normalize_MNIST_images(x):\n",
    "    x = x.astype(np.float32)\n",
    "    x = (x - 127.5)/127.5\n",
    "    return x\n",
    "xtrain = normalize_MNIST_images(xtrain)\n",
    "\n",
    "def label2onehot(lbl):\n",
    "    d = np.zeros((lbl.max() + 1, lbl.size))\n",
    "    d[lbl[np.arange(0, lbl.size)], np.arange(0, lbl.size)] = 1\n",
    "    return d\n",
    "dtrain = label2onehot(ltrain)\n",
    "\n",
    "dtrain = np.transpose(dtrain)\n",
    "print(xtrain.shape)\n",
    "print(dtrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load numpy arrays as tensors onto the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784])\n",
      "torch.Size([60000, 10])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "img = torch.from_numpy(xtrain).float().to(device)\n",
    "lbl = torch.from_numpy(dtrain).float().to(device)\n",
    "'''Below is for mini batch gradient decent '''\n",
    "# train = data_utils.TensorDataset(img,lbl)\n",
    "# data_loader = data_utils.DataLoader(train, batch_size = batch_size, shuffle = True)\n",
    "# for i, (images, labels) in enumerate(data_loader):\n",
    "#     print(images.size(),labels.size())\n",
    "print(img.size())\n",
    "print(lbl.size())\n",
    "print(type(img))\n",
    "print(type(lbl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the hyper parameters and define cross entropy loss and Xavier Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "input_size = 784\n",
    "hidden_size = 64\n",
    "num_classes = 10\n",
    "num_epochs = 200\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "def cross_entropy(input, target):\n",
    "    logsoftmax = nn.LogSoftmax(dim = 1)\n",
    "    return torch.mean(torch.mean(-target * logsoftmax(input), dim=1))\n",
    "\n",
    "def Network_Initialization(model):\n",
    "    if type(model) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(model.weight)\n",
    "        model.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(nn.Linear(input_size, hidden_size),nn.ReLU(),nn.Linear(hidden_size, num_classes),nn.Softmax()).to(device)\n",
    "model.apply(Network_Initialization)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define miss classifications "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss is', 0.2304781973361969)\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "yinit = model(img)\n",
    "print(\"Loss is\", cross_entropy(yinit, lbl).item())\n",
    "\n",
    "def miss_classifications(yinit, lbl):\n",
    "    _, predicted = torch.max(yinit.data, 1)\n",
    "    miss_classifications = (predicted != torch.max(lbl.data, 1)[1]).sum().item()/600\n",
    "    return miss_classifications\n",
    "\n",
    "print(miss_classifications(yinit, lbl)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Miss-Classifications: 89, Loss: 0.230478197336\n",
      "Epoch: 2, Miss-Classifications: 76, Loss: 0.226385489106\n",
      "Epoch: 3, Miss-Classifications: 74, Loss: 0.222975283861\n",
      "Epoch: 4, Miss-Classifications: 68, Loss: 0.220030754805\n",
      "Epoch: 5, Miss-Classifications: 66, Loss: 0.217205613852\n",
      "Epoch: 6, Miss-Classifications: 64, Loss: 0.214683324099\n",
      "Epoch: 7, Miss-Classifications: 63, Loss: 0.212598532438\n",
      "Epoch: 8, Miss-Classifications: 62, Loss: 0.210689052939\n",
      "Epoch: 9, Miss-Classifications: 61, Loss: 0.209145516157\n",
      "Epoch: 10, Miss-Classifications: 59, Loss: 0.207376554608\n",
      "Epoch: 11, Miss-Classifications: 55, Loss: 0.205486416817\n",
      "Epoch: 12, Miss-Classifications: 51, Loss: 0.203228294849\n",
      "Epoch: 13, Miss-Classifications: 49, Loss: 0.201692417264\n",
      "Epoch: 14, Miss-Classifications: 48, Loss: 0.200155705214\n",
      "Epoch: 15, Miss-Classifications: 47, Loss: 0.198153659701\n",
      "Epoch: 16, Miss-Classifications: 47, Loss: 0.197091847658\n",
      "Epoch: 17, Miss-Classifications: 46, Loss: 0.195970028639\n",
      "Epoch: 18, Miss-Classifications: 44, Loss: 0.194452658296\n",
      "Epoch: 19, Miss-Classifications: 40, Loss: 0.192619696259\n",
      "Epoch: 20, Miss-Classifications: 38, Loss: 0.191502124071\n",
      "Epoch: 21, Miss-Classifications: 38, Loss: 0.190764471889\n",
      "Epoch: 22, Miss-Classifications: 37, Loss: 0.189628303051\n",
      "Epoch: 23, Miss-Classifications: 37, Loss: 0.188803657889\n",
      "Epoch: 24, Miss-Classifications: 36, Loss: 0.187596261501\n",
      "Epoch: 25, Miss-Classifications: 34, Loss: 0.186271190643\n",
      "Epoch: 26, Miss-Classifications: 34, Loss: 0.185425683856\n",
      "Epoch: 27, Miss-Classifications: 34, Loss: 0.184873506427\n",
      "Epoch: 28, Miss-Classifications: 33, Loss: 0.184149757028\n",
      "Epoch: 29, Miss-Classifications: 33, Loss: 0.183349251747\n",
      "Epoch: 30, Miss-Classifications: 32, Loss: 0.182640701532\n",
      "Epoch: 31, Miss-Classifications: 32, Loss: 0.182075455785\n",
      "Epoch: 32, Miss-Classifications: 31, Loss: 0.181616917253\n",
      "Epoch: 33, Miss-Classifications: 31, Loss: 0.181153982878\n",
      "Epoch: 34, Miss-Classifications: 31, Loss: 0.180630877614\n",
      "Epoch: 35, Miss-Classifications: 30, Loss: 0.180071890354\n",
      "Epoch: 36, Miss-Classifications: 29, Loss: 0.179424226284\n",
      "Epoch: 37, Miss-Classifications: 27, Loss: 0.178700342774\n",
      "Epoch: 38, Miss-Classifications: 26, Loss: 0.178240284324\n",
      "Epoch: 39, Miss-Classifications: 26, Loss: 0.177644893527\n",
      "Epoch: 40, Miss-Classifications: 25, Loss: 0.176825672388\n",
      "Epoch: 41, Miss-Classifications: 25, Loss: 0.176289081573\n",
      "Epoch: 42, Miss-Classifications: 25, Loss: 0.175664141774\n",
      "Epoch: 43, Miss-Classifications: 24, Loss: 0.174894273281\n",
      "Epoch: 44, Miss-Classifications: 23, Loss: 0.174332484603\n",
      "Epoch: 45, Miss-Classifications: 23, Loss: 0.173948198557\n",
      "Epoch: 46, Miss-Classifications: 22, Loss: 0.173390761018\n",
      "Epoch: 47, Miss-Classifications: 22, Loss: 0.172881290317\n",
      "Epoch: 48, Miss-Classifications: 22, Loss: 0.172569066286\n",
      "Epoch: 49, Miss-Classifications: 22, Loss: 0.172226414084\n",
      "Epoch: 50, Miss-Classifications: 21, Loss: 0.171815127134\n",
      "Epoch: 51, Miss-Classifications: 21, Loss: 0.171503201127\n",
      "Epoch: 52, Miss-Classifications: 21, Loss: 0.171269372106\n",
      "Epoch: 53, Miss-Classifications: 21, Loss: 0.171007424593\n",
      "Epoch: 54, Miss-Classifications: 21, Loss: 0.1707418859\n",
      "Epoch: 55, Miss-Classifications: 21, Loss: 0.170520171523\n",
      "Epoch: 56, Miss-Classifications: 21, Loss: 0.170330807567\n",
      "Epoch: 57, Miss-Classifications: 20, Loss: 0.170134216547\n",
      "Epoch: 58, Miss-Classifications: 20, Loss: 0.169923603535\n",
      "Epoch: 59, Miss-Classifications: 20, Loss: 0.169744327664\n",
      "Epoch: 60, Miss-Classifications: 20, Loss: 0.169596910477\n",
      "Epoch: 61, Miss-Classifications: 20, Loss: 0.169431731105\n",
      "Epoch: 62, Miss-Classifications: 20, Loss: 0.169256299734\n",
      "Epoch: 63, Miss-Classifications: 20, Loss: 0.169109538198\n",
      "Epoch: 64, Miss-Classifications: 20, Loss: 0.168978437781\n",
      "Epoch: 65, Miss-Classifications: 19, Loss: 0.168838754296\n",
      "Epoch: 66, Miss-Classifications: 19, Loss: 0.168699920177\n",
      "Epoch: 67, Miss-Classifications: 19, Loss: 0.168574541807\n",
      "Epoch: 68, Miss-Classifications: 19, Loss: 0.168457463384\n",
      "Epoch: 69, Miss-Classifications: 19, Loss: 0.168339312077\n",
      "Epoch: 70, Miss-Classifications: 19, Loss: 0.168219640851\n",
      "Epoch: 71, Miss-Classifications: 19, Loss: 0.168107703328\n",
      "Epoch: 72, Miss-Classifications: 19, Loss: 0.16800673306\n",
      "Epoch: 73, Miss-Classifications: 19, Loss: 0.167906165123\n",
      "Epoch: 74, Miss-Classifications: 19, Loss: 0.167804002762\n",
      "Epoch: 75, Miss-Classifications: 19, Loss: 0.167708516121\n",
      "Epoch: 76, Miss-Classifications: 19, Loss: 0.167618528008\n",
      "Epoch: 77, Miss-Classifications: 19, Loss: 0.167529135942\n",
      "Epoch: 78, Miss-Classifications: 19, Loss: 0.167442768812\n",
      "Epoch: 79, Miss-Classifications: 19, Loss: 0.167360842228\n",
      "Epoch: 80, Miss-Classifications: 18, Loss: 0.167280361056\n",
      "Epoch: 81, Miss-Classifications: 18, Loss: 0.167201191187\n",
      "Epoch: 82, Miss-Classifications: 18, Loss: 0.167124956846\n",
      "Epoch: 83, Miss-Classifications: 18, Loss: 0.167051851749\n",
      "Epoch: 84, Miss-Classifications: 18, Loss: 0.166981130838\n",
      "Epoch: 85, Miss-Classifications: 18, Loss: 0.166911318898\n",
      "Epoch: 86, Miss-Classifications: 18, Loss: 0.166843101382\n",
      "Epoch: 87, Miss-Classifications: 18, Loss: 0.166778206825\n",
      "Epoch: 88, Miss-Classifications: 18, Loss: 0.166714817286\n",
      "Epoch: 89, Miss-Classifications: 18, Loss: 0.166651412845\n",
      "Epoch: 90, Miss-Classifications: 18, Loss: 0.166590005159\n",
      "Epoch: 91, Miss-Classifications: 18, Loss: 0.16653136909\n",
      "Epoch: 92, Miss-Classifications: 18, Loss: 0.166473671794\n",
      "Epoch: 93, Miss-Classifications: 18, Loss: 0.166416332126\n",
      "Epoch: 94, Miss-Classifications: 18, Loss: 0.16636043787\n",
      "Epoch: 95, Miss-Classifications: 18, Loss: 0.166306331754\n",
      "Epoch: 96, Miss-Classifications: 18, Loss: 0.166253566742\n",
      "Epoch: 97, Miss-Classifications: 18, Loss: 0.166201487184\n",
      "Epoch: 98, Miss-Classifications: 18, Loss: 0.166150301695\n",
      "Epoch: 99, Miss-Classifications: 18, Loss: 0.166100680828\n",
      "Epoch: 100, Miss-Classifications: 18, Loss: 0.166051983833\n",
      "Epoch: 101, Miss-Classifications: 18, Loss: 0.16600382328\n",
      "Epoch: 102, Miss-Classifications: 18, Loss: 0.165956899524\n",
      "Epoch: 103, Miss-Classifications: 17, Loss: 0.165910989046\n",
      "Epoch: 104, Miss-Classifications: 17, Loss: 0.16586586833\n",
      "Epoch: 105, Miss-Classifications: 17, Loss: 0.165821552277\n",
      "Epoch: 106, Miss-Classifications: 17, Loss: 0.165778174996\n",
      "Epoch: 107, Miss-Classifications: 17, Loss: 0.16573561728\n",
      "Epoch: 108, Miss-Classifications: 17, Loss: 0.165693804622\n",
      "Epoch: 109, Miss-Classifications: 17, Loss: 0.165652751923\n",
      "Epoch: 110, Miss-Classifications: 17, Loss: 0.165612474084\n",
      "Epoch: 111, Miss-Classifications: 17, Loss: 0.165573015809\n",
      "Epoch: 112, Miss-Classifications: 17, Loss: 0.16553413868\n",
      "Epoch: 113, Miss-Classifications: 17, Loss: 0.165495887399\n",
      "Epoch: 114, Miss-Classifications: 17, Loss: 0.165458485484\n",
      "Epoch: 115, Miss-Classifications: 17, Loss: 0.165421590209\n",
      "Epoch: 116, Miss-Classifications: 17, Loss: 0.165385305882\n",
      "Epoch: 117, Miss-Classifications: 17, Loss: 0.165349721909\n",
      "Epoch: 118, Miss-Classifications: 17, Loss: 0.165314719081\n",
      "Epoch: 119, Miss-Classifications: 17, Loss: 0.165280163288\n",
      "Epoch: 120, Miss-Classifications: 17, Loss: 0.165246188641\n",
      "Epoch: 121, Miss-Classifications: 17, Loss: 0.165212795138\n",
      "Epoch: 122, Miss-Classifications: 17, Loss: 0.165179908276\n",
      "Epoch: 123, Miss-Classifications: 17, Loss: 0.165147453547\n",
      "Epoch: 124, Miss-Classifications: 17, Loss: 0.165115505457\n",
      "Epoch: 125, Miss-Classifications: 17, Loss: 0.165084064007\n",
      "Epoch: 126, Miss-Classifications: 17, Loss: 0.165053039789\n",
      "Epoch: 127, Miss-Classifications: 17, Loss: 0.165022447705\n",
      "Epoch: 128, Miss-Classifications: 17, Loss: 0.164992302656\n",
      "Epoch: 129, Miss-Classifications: 17, Loss: 0.164962604642\n",
      "Epoch: 130, Miss-Classifications: 17, Loss: 0.164933249354\n",
      "Epoch: 131, Miss-Classifications: 17, Loss: 0.164904266596\n",
      "Epoch: 132, Miss-Classifications: 17, Loss: 0.164875715971\n",
      "Epoch: 133, Miss-Classifications: 17, Loss: 0.164847567677\n",
      "Epoch: 134, Miss-Classifications: 17, Loss: 0.164819732308\n",
      "Epoch: 135, Miss-Classifications: 17, Loss: 0.164792254567\n",
      "Epoch: 136, Miss-Classifications: 17, Loss: 0.164765134454\n",
      "Epoch: 137, Miss-Classifications: 17, Loss: 0.164738386869\n",
      "Epoch: 138, Miss-Classifications: 17, Loss: 0.164711952209\n",
      "Epoch: 139, Miss-Classifications: 17, Loss: 0.164685860276\n",
      "Epoch: 140, Miss-Classifications: 17, Loss: 0.164660051465\n",
      "Epoch: 141, Miss-Classifications: 17, Loss: 0.164634585381\n",
      "Epoch: 142, Miss-Classifications: 17, Loss: 0.16460943222\n",
      "Epoch: 143, Miss-Classifications: 17, Loss: 0.164584562182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 144, Miss-Classifications: 17, Loss: 0.164559990168\n",
      "Epoch: 145, Miss-Classifications: 17, Loss: 0.164535671473\n",
      "Epoch: 146, Miss-Classifications: 17, Loss: 0.164511725307\n",
      "Epoch: 147, Miss-Classifications: 16, Loss: 0.164488002658\n",
      "Epoch: 148, Miss-Classifications: 16, Loss: 0.16446454823\n",
      "Epoch: 149, Miss-Classifications: 16, Loss: 0.164441376925\n",
      "Epoch: 150, Miss-Classifications: 16, Loss: 0.164418444037\n",
      "Epoch: 151, Miss-Classifications: 16, Loss: 0.164395779371\n",
      "Epoch: 152, Miss-Classifications: 16, Loss: 0.16437330842\n",
      "Epoch: 153, Miss-Classifications: 16, Loss: 0.164351120591\n",
      "Epoch: 154, Miss-Classifications: 16, Loss: 0.164329171181\n",
      "Epoch: 155, Miss-Classifications: 16, Loss: 0.164307489991\n",
      "Epoch: 156, Miss-Classifications: 16, Loss: 0.164285987616\n",
      "Epoch: 157, Miss-Classifications: 16, Loss: 0.164264693856\n",
      "Epoch: 158, Miss-Classifications: 16, Loss: 0.164243683219\n",
      "Epoch: 159, Miss-Classifications: 16, Loss: 0.164222836494\n",
      "Epoch: 160, Miss-Classifications: 16, Loss: 0.164202228189\n",
      "Epoch: 161, Miss-Classifications: 16, Loss: 0.164181828499\n",
      "Epoch: 162, Miss-Classifications: 16, Loss: 0.164161577821\n",
      "Epoch: 163, Miss-Classifications: 16, Loss: 0.164141565561\n",
      "Epoch: 164, Miss-Classifications: 16, Loss: 0.164121732116\n",
      "Epoch: 165, Miss-Classifications: 16, Loss: 0.164102092385\n",
      "Epoch: 166, Miss-Classifications: 16, Loss: 0.164082631469\n",
      "Epoch: 167, Miss-Classifications: 16, Loss: 0.164063364267\n",
      "Epoch: 168, Miss-Classifications: 16, Loss: 0.164044246078\n",
      "Epoch: 169, Miss-Classifications: 16, Loss: 0.164025336504\n",
      "Epoch: 170, Miss-Classifications: 16, Loss: 0.164006575942\n",
      "Epoch: 171, Miss-Classifications: 16, Loss: 0.163987994194\n",
      "Epoch: 172, Miss-Classifications: 16, Loss: 0.163969561458\n",
      "Epoch: 173, Miss-Classifications: 16, Loss: 0.163951292634\n",
      "Epoch: 174, Miss-Classifications: 16, Loss: 0.163933187723\n",
      "Epoch: 175, Miss-Classifications: 16, Loss: 0.16391518712\n",
      "Epoch: 176, Miss-Classifications: 16, Loss: 0.163897395134\n",
      "Epoch: 177, Miss-Classifications: 16, Loss: 0.163879707456\n",
      "Epoch: 178, Miss-Classifications: 16, Loss: 0.163862198591\n",
      "Epoch: 179, Miss-Classifications: 16, Loss: 0.163844794035\n",
      "Epoch: 180, Miss-Classifications: 16, Loss: 0.163827523589\n",
      "Epoch: 181, Miss-Classifications: 16, Loss: 0.163810431957\n",
      "Epoch: 182, Miss-Classifications: 16, Loss: 0.163793444633\n",
      "Epoch: 183, Miss-Classifications: 16, Loss: 0.16377659142\n",
      "Epoch: 184, Miss-Classifications: 16, Loss: 0.163759827614\n",
      "Epoch: 185, Miss-Classifications: 16, Loss: 0.163743197918\n",
      "Epoch: 186, Miss-Classifications: 16, Loss: 0.163726717234\n",
      "Epoch: 187, Miss-Classifications: 16, Loss: 0.163710281253\n",
      "Epoch: 188, Miss-Classifications: 16, Loss: 0.163694053888\n",
      "Epoch: 189, Miss-Classifications: 16, Loss: 0.163677930832\n",
      "Epoch: 190, Miss-Classifications: 16, Loss: 0.163661882281\n",
      "Epoch: 191, Miss-Classifications: 16, Loss: 0.163646012545\n",
      "Epoch: 192, Miss-Classifications: 16, Loss: 0.163630247116\n",
      "Epoch: 193, Miss-Classifications: 16, Loss: 0.163614600897\n",
      "Epoch: 194, Miss-Classifications: 16, Loss: 0.163599044085\n",
      "Epoch: 195, Miss-Classifications: 16, Loss: 0.163583576679\n",
      "Epoch: 196, Miss-Classifications: 16, Loss: 0.163568228483\n",
      "Epoch: 197, Miss-Classifications: 16, Loss: 0.163553014398\n",
      "Epoch: 198, Miss-Classifications: 16, Loss: 0.16353790462\n",
      "Epoch: 199, Miss-Classifications: 16, Loss: 0.163522914052\n",
      "Epoch: 200, Miss-Classifications: 16, Loss: 0.16350799799\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(img)\n",
    "    loss = cross_entropy(outputs,lbl)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    mc = miss_classifications(outputs, lbl)\n",
    "    print ('Epoch: {}, Miss-Classifications: {}, Loss: {}'.format(epoch+1, mc , loss.item()))\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
